{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-78fc683d5810>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  enc.fit(y[:,np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-fd377ab0d802>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  Y = enc.transform(y[:,np.newaxis]).toarray()\n"
     ]
    }
   ],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax 공식\n",
    "\n",
    "$$ \n",
    "diag(A_{exp} \\mathbb{1}_k)^{-1} A_{exp} \n",
    "= \\begin{bmatrix}\n",
    "{1 \\over \\sum_{j}exp(a_{1j})} & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & {1 \\over \\sum_{j}exp(a_{Nj})}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "- \\ {exp(a_1^T)} \\ - \\\\ \\vdots\n",
    "\\end{bmatrix}\n",
    "= Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E(w) 공식\n",
    "\n",
    "$$ E(w) = {1 \\over N} \\sum_{n=1}^{N} {E_n{w}} = -{1 \\over N} \\sum_{n=1}{N} \\sum_{k=1}^{K} t_{nk} \\ln{y_{nk}} = -{1 \\over N} \\{ \\mathbb{1}_N^T (\\ln(Y) \\circ T) \\mathbb{1}_k \\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, lambda_=0):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    \n",
    "    reg_l2 = (lambda_ / 2) *  np.sum(np.dot(W.T, W))\n",
    "    \n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1)) + reg_l2\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W의 Update 공식\n",
    "\n",
    "$$ W = W - \\{learning\\_rate * {{1 \\over N} \\Phi^T (Y - T)}\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size, val_ratio, lambda_):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations+1,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_total    = X[shuffled_indices]\n",
    "    T_total    = T[shuffled_indices]\n",
    "    \n",
    "    val_N      = int(N * (1 - val_ratio))\n",
    "    X_shuffled = X_total[:val_N]\n",
    "    T_shuffled = T_total[:val_N]\n",
    "    X_eval     = X_total[val_N:]\n",
    "    T_eval     = T_total[val_N:]\n",
    "\n",
    "    for i in range(iterations+1):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "            \n",
    "        ## Update\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, lambda_)\n",
    "        if i % 10000 == 0:\n",
    "            print(\"ep :\", i, cost_history[i][0], compute_cost(X_eval, T_eval, W, lambda_))\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> lambda : 0\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2819687302428764 [[2.29253048]]\n",
      "ep : 10000 0.3008118929597312 [[0.38656275]]\n",
      "ep : 20000 0.4447022748071709 [[0.3511305]]\n",
      "ep : 30000 0.20286393122143603 [[0.33815048]]\n",
      "ep : 40000 0.43355588100228193 [[0.3281135]]\n",
      "ep : 50000 0.032138960771842764 [[0.42100292]]\n",
      "\n",
      ">> lambda : 1e-06\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2815077077996357 [[2.29118186]]\n",
      "ep : 10000 0.18662060560379562 [[0.37738019]]\n",
      "ep : 20000 0.36443268697637227 [[0.34221517]]\n",
      "ep : 30000 0.25436704765714513 [[0.32104692]]\n",
      "ep : 40000 0.3129903183990167 [[0.31250543]]\n",
      "ep : 50000 0.029317246023465634 [[0.38386871]]\n",
      "\n",
      ">> lambda : 1e-05\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2696098602217756 [[2.29243081]]\n",
      "ep : 10000 0.6203805344682827 [[0.37195958]]\n",
      "ep : 20000 0.21336199165159864 [[0.33822009]]\n",
      "ep : 30000 0.4060430679151972 [[0.31366937]]\n",
      "ep : 40000 0.2502876041782785 [[0.31620549]]\n",
      "ep : 50000 0.02542215244103693 [[0.34896378]]\n",
      "\n",
      ">> lambda : 0.0001\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2800613742226328 [[2.29215556]]\n",
      "ep : 10000 0.2080924862402249 [[0.38645431]]\n",
      "ep : 20000 0.2764946683862383 [[0.36239381]]\n",
      "ep : 30000 0.31321714950189333 [[0.33049673]]\n",
      "ep : 40000 0.19701055028219847 [[0.3293485]]\n",
      "ep : 50000 0.026754663181954977 [[0.38042179]]\n",
      "\n",
      ">> lambda : 0.001\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2782144948413587 [[2.29138838]]\n",
      "ep : 10000 0.20708890978510863 [[0.39406667]]\n",
      "ep : 20000 0.2164092432640204 [[0.34422588]]\n",
      "ep : 30000 0.22659315947950792 [[0.34316365]]\n",
      "ep : 40000 0.36867736539257145 [[0.32713565]]\n",
      "ep : 50000 0.01986176768000006 [[0.45524546]]\n",
      "\n",
      ">> lambda : 0.01\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.278119465869412 [[2.29033995]]\n",
      "ep : 10000 0.25571505244526577 [[0.37775554]]\n",
      "ep : 20000 0.18447412840090424 [[0.35158513]]\n",
      "ep : 30000 0.20232933496984568 [[0.33810019]]\n",
      "ep : 40000 0.2134386097064828 [[0.32931224]]\n",
      "ep : 50000 0.02414981066108749 [[0.35391973]]\n",
      "\n",
      ">> lambda : 0.1\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2798168357543775 [[2.29148415]]\n",
      "ep : 10000 0.3543489988257095 [[0.39114021]]\n",
      "ep : 20000 0.30046536645896427 [[0.35216709]]\n",
      "ep : 30000 0.47405410410297283 [[0.34348632]]\n",
      "ep : 40000 0.34582589161219296 [[0.33235808]]\n",
      "ep : 50000 0.03129053892944444 [[0.40519734]]\n",
      "\n",
      ">> lambda : 0.5\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2807617713567874 [[2.29168226]]\n",
      "ep : 10000 0.4507572716441338 [[0.3938984]]\n",
      "ep : 20000 0.19637629901952824 [[0.34553046]]\n",
      "ep : 30000 0.17966685507179425 [[0.35485315]]\n",
      "ep : 40000 0.26268767944220495 [[0.3253673]]\n",
      "ep : 50000 0.01981974125825446 [[0.36365357]]\n",
      "\n",
      ">> lambda : 0.8\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2780365109991614 [[2.29135665]]\n",
      "ep : 10000 0.27799012741804074 [[0.3667548]]\n",
      "ep : 20000 0.28039785640427656 [[0.33618117]]\n",
      "ep : 30000 0.3274102861870482 [[0.32952612]]\n",
      "ep : 40000 0.22001323893493507 [[0.31639784]]\n",
      "ep : 50000 0.027421842302742992 [[0.44757318]]\n",
      "\n",
      ">> lambda : 1\n",
      "Initial Cost is: 2.3024850979937352\n",
      "ep : 0 2.2734645294738507 [[2.2899685]]\n",
      "ep : 10000 0.28599700222657354 [[0.38748656]]\n",
      "ep : 20000 0.32182968329298617 [[0.3384586]]\n",
      "ep : 30000 0.25264649059853406 [[0.33906674]]\n",
      "ep : 40000 0.3179945186251854 [[0.32152067]]\n",
      "ep : 50000 0.017763444654318724 [[0.36394441]]\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.5, 0.8, 1]\n",
    "\n",
    "for l in lambdas : #{\n",
    "    X = np.hstack((np.ones((np.size(X_train, 0),1)), X_train))\n",
    "    T = y_train\n",
    "\n",
    "    K = np.size(T, 1)\n",
    "    M = np.size(X, 1)\n",
    "    W = np.zeros((M,K))\n",
    "\n",
    "    iterations    = 50000\n",
    "    learning_rate = 0.01\n",
    "    val_ratio     = 0.2\n",
    "    lambda_       = 0\n",
    "\n",
    "    initial_cost  = compute_cost(X, T, W)\n",
    "\n",
    "    print(f'\\n>> lambda : {l}')\n",
    "    print(\"Initial Cost is: {}\".format(initial_cost[0][0]))\n",
    "\n",
    "    (cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64, val_ratio, lambda_)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.3024850979937352 \n",
      "\n",
      "ep : 0 2.281500293625614 [[2.29271853]]\n",
      "ep : 10000 0.3333691120750739 [[0.39291427]]\n",
      "ep : 20000 0.30398021754341126 [[0.33905457]]\n",
      "ep : 30000 0.4718158897978205 [[0.31760922]]\n",
      "ep : 40000 0.2872860418810027 [[0.31942722]]\n",
      "ep : 50000 0.023277707670544018 [[0.35606077]]\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)), X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations    = 50000\n",
    "learning_rate = 0.01\n",
    "val_ratio     = 0.2\n",
    "lambda_       = 1e-5\n",
    "\n",
    "initial_cost  = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64, val_ratio, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
